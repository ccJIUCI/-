{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "36dfea85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup \n",
    "import random\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.action_chains import ActionChains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d6c6aa6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_imag(url,save_path):\n",
    "    browser = selenium.webdriver.Chrome() # 打开谷歌浏览器\n",
    "    browser.get(url)\n",
    "    for i in range(1,20):\n",
    "        time.sleep(0.2)#稍微停顿\n",
    "        browser.execute_script(\"window.scrollBy(0,2000)\")        \n",
    "    node = browser.find_elements_by_xpath(\"//div[@id = 'J-detail-content']//img\") # 寻找节点\n",
    "    j=0\n",
    "    if node!=[] :\n",
    "        print(1)\n",
    "        for i in node:\n",
    "            j=j+1#备用图片名\n",
    "            with open(save_path.rstrip('/')+'/'+i.get_attribute('src')[-5:],'wb') as f:\n",
    "                img_url=i.get_attribute('src')\n",
    "                print(img_url)\n",
    "                headers={\n",
    "                    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.84 Safari/537.36'\n",
    "                }\n",
    "                img_get=requests.get(img_url,headers=headers).content\n",
    "                f.write(img_get)\n",
    "    else:\n",
    "        print(2)\n",
    "        node = browser.find_elements_by_xpath(\"//div[@id = 'J-detail-content']/div/div\")\n",
    "        id_list=[]#图片id名\n",
    "        for i in node:\n",
    "            id_list.append(i.get_attribute('data-id'))\n",
    "        style_inf=browser.find_element_by_xpath(\"//div[@id = 'J-detail-content']//style\")\n",
    "        style=style_inf.get_attribute('innerHTML')\n",
    "        style=str(style)\n",
    "        while(style.find('background-image:url(')!=-1):\n",
    "            start=(style.find('background-image:url(')+len('background-image:url('))\n",
    "            style=style[start:]\n",
    "            end=style.find(');')\n",
    "            img_url=style[:end]\n",
    "            img_url='https:'+img_url\n",
    "            with open(save_path.rstrip('/')+'/'+img_url[-10:],'wb') as f:\n",
    "                headers={\n",
    "                    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.84 Safari/537.36'\n",
    "                }\n",
    "                img_get=requests.get(img_url,headers=headers).content\n",
    "                f.write(img_get)\n",
    "    browser.quit()\n",
    "    print('Finish!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "64fbba18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_all_image(ans,page):\n",
    "    Headers={\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.74 Safari/537.36 Edg/99.0.1150.55\",\n",
    "        \"cookie\": \"__jdv=122270672|direct|-|none|-|1647493502013; __jdu=164749350201356425815; shshshfpa=378323e4-cf10-6ab8-6f67-85eef54ba88b-1647493502; shshshfpb=w5GiBgnUHADfTmx7Wqeau-g; shshshfp=422ab1959481b6e38d8604bd9328dac8; ip_cityCode=549; areaId=7; ipLoc-djd=7-549-558-34702; __jda=122270672.164749350201356425815.1647493502.1648606921.1648646379.4; __jdc=122270672; token=918c1e81d67b5ab3bffc32e8bfe81751,2,915915; __tk=a099cac6b2821d27822c62eda2051af9,2,915915; __jdb=122270672.4.164749350201356425815|4.1648646379; shshshsID=7cf398889fcc32804818069ed03e31ec_4_1648647771748; 3AB9D23F7A4B3C9B=WAHUHBX3N5DU25OGXJGSSITEBFRQSR67LQEBLPZPSFMJTI5D3SWLLRKOQAUZNGAJKEFGYMKO4DLNMQC352FH22VDYU\"\n",
    "    }\n",
    "    lst_products = []\n",
    "    allimg_url={}#存放全部图书图片的url\n",
    "    for pp in range(1,page+1):\n",
    "        url = 'https://search.jd.com/Search?keyword={}&page={}'.format(ans,pp)\n",
    "        print(url)\n",
    "        res = requests.get(url, headers=Headers)# requests.post(url, headers=Headers)\n",
    "        res.encoding = res.apparent_encoding\n",
    "        html = res.text\n",
    "        #DOM\n",
    "        soup = BeautifulSoup(html,'html5lib')\n",
    "        lst_li = soup.find('div',id=\"J_goodsList\").find_all('li')\n",
    "        for idx, li in enumerate(lst_li):\n",
    "            #time.sleep(10)#睡眠一秒钟\n",
    "            print(idx)\n",
    "#             if idx in [0]:\n",
    "#                 continue;\n",
    "            product = []\n",
    "            img_url=[]\n",
    "            try:\n",
    "                product_id = li['data-sku']\n",
    "                product_price = li.find('div',class_=\"p-price\").strong.i.string.strip()\n",
    "                product_name =  \"\".join(li.find('div',class_= \"p-name\").a.em.strings)\n",
    "                product.append(product_id)\n",
    "                product.append(product_price)\n",
    "                product.append(product_name)\n",
    "                allimg_url[product_name]=[]\n",
    "            except:\n",
    "                print(\"Error in\", li )\n",
    "\n",
    "            try:\n",
    "                url_cm = 'https://club.jd.com/comment/productPageComments.action?callback=fetchJSON_comment98&productId={}&score=0&sortType=5&page=1&pageSize=10&isShadowSku=0&rid=0&fold=1'.format(product_id)\n",
    "                res = requests.get(url_cm,headers=Headers)\n",
    "                jd = json.loads(res.text.lstrip('fetchJSON_comment98vv12345(').rstrip(');'))\n",
    "                data_list = jd['comments']\n",
    "                comments = \" \".join([data['content'] for data in data_list])\n",
    "                product.append(comments)\n",
    "                    ##这里是保存图片的url\n",
    "                for i in range(len(jd['comments'])):\n",
    "                    if 'images' in jd['comments'][i].keys():\n",
    "                        for j in range(len(jd['comments'][i]['images'])):\n",
    "                            allimg_url[product_name].append(jd['comments'][i]['images'][j]['imgUrl'])\n",
    "                            img_url.append(jd['comments'][i]['images'][j]['imgUrl'])\n",
    "                #爬取完一本图书，开始进行图片的保存,创建以该图书为名称的文件夹，下设买家秀和卖家秀文件夹\n",
    "                path='D:\\img_save'\n",
    "                if not os.path.exists(path):\n",
    "                    os.mkdir(path)\n",
    "                if os.path.exists(path):\n",
    "                    os.makedirs(path+'\\\\'+product_name)\n",
    "                    os.makedirs(path+'\\\\'+product_name+'\\\\'+'卖家秀')\n",
    "                    os.makedirs(path+'\\\\'+product_name+'\\\\'+'买家秀')\n",
    "            #保存卖家秀的图片\n",
    "                sell_url=\"https://item.jd.com/{}.html#none\".format(product_id)\n",
    "                print(sell_url)\n",
    "                savepath=path+'\\\\'+product_name+'\\\\'+'卖家秀'\n",
    "                print(savepath)\n",
    "                get_imag(sell_url,savepath)\n",
    "            #保存买家秀的图片\n",
    "                for i in range(len(img_url)):\n",
    "                    picName = img_url[i].split('/')[-1]#图片的名称\n",
    "                    response = requests.get('http:'+img_url[i],headers=Headers)\n",
    "                    with open(path+'\\\\'+product_name+'\\\\'+'买家秀'+'\\\\'+picName,'wb') as f:\n",
    "                        f.write(response.content)\n",
    "            except:\n",
    "                print(\"Error in Comments\", product_id)\n",
    "            lst_products.append(product)\n",
    "    print(lst_products)\n",
    "    pd.DataFrame.from_dict(allimg_url, orient='index').to_csv('JD_product_imgurl.csv',encoding='utf-8-sig')\n",
    "    pd.DataFrame(lst_products,columns =['product_id','product_id_price','product_id_name','product_id_comments']).to_csv('JD_product_ids.csv',encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9f3eb913",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://search.jd.com/Search?keyword=python&page=1\n",
      "0\n",
      "https://item.jd.com/11993134.html#none\n",
      "D:\\img_save\\Python编程 从入门到实践 第2版（图灵出品）\\卖家秀\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Temp/ipykernel_14340/1083889334.py:7: DeprecationWarning: find_elements_by_* commands are deprecated. Please use find_elements() instead\n",
      "  node = browser.find_elements_by_xpath(\"//div[@id = 'J-detail-content']//img\") # 寻找节点\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "https://img30.360buyimg.com/vc/jfs/t27199/364/2343978207/110813/41e64e8d/5bff723dN829c479c.jpg\n",
      "https://img30.360buyimg.com/vc/jfs/t1/85183/27/22408/195783/6220e1f2E90f673de/dace7c922f1922be.jpg\n",
      "https://img30.360buyimg.com/vc/jfs/t1/200419/14/14674/615516/6178c11eEfc126148/eeb610b28aedac63.jpg\n",
      "Finish!\n",
      "https://search.jd.com/Search?keyword=python&page=2\n",
      "0\n",
      "https://item.jd.com/10043837952779.html#none\n",
      "D:\\img_save\\京东云&千锋教育联合共创 Python教程自学全套视频教程基础入门人工智能AI爬虫课 Python全栈开发+人工智能+数据分析+就业推荐\\卖家秀\n",
      "1\n",
      "https://img10.360buyimg.com/imgzone/jfs/t1/212072/2/11393/1079827/61ea2050E60c6d2be/8f841d29bb8e4308.jpg\n",
      "https://img10.360buyimg.com/imgzone/jfs/t1/90130/17/21707/459232/61ea2050E10357c98/4503e3cc786b0efc.jpg\n",
      "https://img10.360buyimg.com/imgzone/jfs/t1/142279/11/26863/359835/61ea2050Ea8d07944/bda4439f5c4cdea5.jpg\n",
      "https://img10.360buyimg.com/imgzone/jfs/t1/104338/8/22437/1187674/61ea2051E95dc4bce/15c3394fbcd1ee39.jpg\n",
      "https://img10.360buyimg.com/imgzone/jfs/t1/98240/3/22724/328375/61ea2050E0dcf7dd8/288f5be381b00e5a.jpg\n",
      "https://img10.360buyimg.com/imgzone/jfs/t1/223534/36/3577/505122/61ea2051Ee4be1923/c86e78585ef9d4f1.jpg\n",
      "https://img10.360buyimg.com/imgzone/jfs/t1/102190/2/22390/480258/61ea2051E24aae655/12ba3fd047791f9e.jpg\n",
      "https://img10.360buyimg.com/imgzone/jfs/t1/101826/24/22089/480258/61ea2051E91334e03/beb3a8572a532063.jpg\n",
      "Finish!\n",
      "[['11993134', '73.60', 'Python编程 从入门到实践 第2版（图灵出品）', '看图书销量榜，买来的书，准备自学编程，听说python很适合入门者，便买了这本书，感觉还是不错的，京东快递是用箱子装过来的，图书没有一点损坏，很不错，这次的满减活动，价格划算，值得购买！！ 很好的python入门书籍，非常适合初学者，买回来给初中的小朋友用的，讲解比较详细，还有一本小的习题册，每一章节安排了一些小练习，巩固所学知识！值得购买，推荐！ 孩子要买的,印刷的质量很好,说教程不错,关键是正版可以放心学,不像盗版,书中可能有错误.在京东上购买放心,好东西值得购买. 强烈推荐这边，内容简单，关键排版舒服，思路清晰，各种逻辑清晰，非常好。不像我现在看的什么深入浅出pandas，纯粹是坑 纸质优良，印刷得很清晰，可以作为工具书仔细研读，查阅使用。内容很详尽，适合循序渐进的学习。很惭愧我现在只看了一点皮毛，后续还需要努力 纸张质感非常顺滑，厚度适中，第一次买编程方面的书籍。步骤很详细，还有练习。希望可以按部就班的学习一下，看看有没有能力学会一点。好评！ 还不错，印书质量不错，还行，就是有点小贵， 正品，买的放心，价钱也公道，满意！真的推荐啊，快递速度特别的快，爱了爱了很快就到了，全新正品?，包装很仔细，没有问题物流满分?，包装快递满分?，配送员态度满分?，购物让人开心，吃土使我快乐~?????????当时买的时候还在犹豫价格，买来之后一点都不后悔，而且物流还快，可以入手很实用，虽然没书之前都会了，可以反复查看加深记忆 而且从包装来看，非常的严密，不论遇上怎样的天气，书都不会坏掉\\n\\n从书本身来看，印刷质量非常好，是正品，内容涵盖非常全面，值得入手 看腰封就知道，这本书已然经典之作，无论是入门，还是老程序员当成枕边书或当作手册查阅都是OK的，内容很棒，错误少，真正的用心在写。我把以前的卖掉又入了2版百万册纪念版。'], ['10043837952779', '9880.00', '京东云&千锋教育联合共创 Python教程自学全套视频教程基础入门人工智能AI爬虫课 Python全栈开发+人工智能+数据分析+就业推荐', '']]\n"
     ]
    }
   ],
   "source": [
    "get_all_image('python',1) #第一个参数是要查找的内容，第二个参数是要从头开始查找的页数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d895714",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
